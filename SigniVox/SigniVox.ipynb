{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahA-zwfMswvy"
      },
      "outputs": [],
      "source": [
        "!pip install googletrans==3.1.0a0\n",
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip uninstall transformers\n",
        "!pip install -q git+https://github.com/zphang/transformers@c3dc391\n",
        "!pip -q install git+https://github.com/huggingface/peft.git\n",
        "!pip -q install bitsandbytes\n",
        "!pip install gradio\n",
        "#!pip install git+https://github.com/sanchit-gandhi/whisper-jax.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "import librosa\n",
        "from googletrans import Translator\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "from peft import PeftModel\n",
        "from transformers import LLaMATokenizer, LLaMAForCausalLM, GenerationConfig\n",
        "import textwrap\n",
        "\n"
      ],
      "metadata": {
        "id": "kmZgIKzCtKVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "#from datasets import load_dataset\n",
        "\n",
        "# load model and processor\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium\")\n",
        "audio_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-medium\")\n",
        "\n"
      ],
      "metadata": {
        "id": "4KEICN-PtU2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = LLaMATokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
        "\n",
        "model = LLaMAForCausalLM.from_pretrained(\"decapoda-research/llama-7b-hf\",load_in_8bit=True,device_map=\"auto\",)\n",
        "model = PeftModel.from_pretrained(model, \"samwit/alpaca7B-lora\")"
      ],
      "metadata": {
        "id": "ymFf3Ad-tV1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "file_path = 'test.mp3'\n",
        "audio_data, sample_rate = librosa.load(file_path)\n",
        "# Target sample rate (16000 Hz in this case)\n",
        "target_sample_rate = 16000\n",
        "# Resample the audio data to the target sample rate\n",
        "audio_data_resampled = librosa.resample(audio_data, orig_sr=sample_rate, target_sr=target_sample_rate)\n"
      ],
      "metadata": {
        "id": "61_25K4Ntft5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "batch_length_sec = 30\n",
        "sample_rate = 16000\n",
        "batch_length_samples = batch_length_sec * sample_rate\n",
        "\n",
        "L =[]\n",
        "\n",
        "audio_batches = [audio_data_resampled[i:i+batch_length_samples] for i in range(0, len(audio_data_resampled), batch_length_samples)]\n",
        "for batch in tqdm(audio_batches):\n",
        "    input_features = processor(batch, sampling_rate=target_sample_rate, return_tensors=\"pt\").input_features\n",
        "\n",
        "    # generate token ids\n",
        "    predicted_ids = audio_model.generate(input_features)\n",
        "    # decode token ids to text\n",
        "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\n",
        "\n",
        "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "\n",
        "    L.append(transcription)\n",
        "\n",
        "\n",
        "L= [b[0] for b in L]"
      ],
      "metadata": {
        "id": "hUGMtoowt0ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text= \" \".join(L)"
      ],
      "metadata": {
        "id": "oWQuWfod4vdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator()"
      ],
      "metadata": {
        "id": "SFMgUTkh5iIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = translator.translate(text, dest='en')\n",
        "text = text.text\n",
        "with open('test.txt', 'w') as f:\n",
        "    f.write(text)"
      ],
      "metadata": {
        "id": "WU9iV37aKVt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(text):\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    input_ids = inputs[\"input_ids\"].cuda()\n",
        "\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=0.1,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.2,\n",
        "    )\n",
        "    print(\"Generating...\")\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=256,\n",
        "    )\n",
        "    for s in generation_output.sequences:\n",
        "        a = tokenizer.decode(s)\n",
        "\n",
        "    return tokenizer.decode(s)"
      ],
      "metadata": {
        "id": "Ll2SIzIJuK4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatBot(question):\n",
        "\n",
        "  ques = translator.translate(question, dest='en')\n",
        "  question = ques.text\n",
        "  with open('test.txt') as f:\n",
        "    contents = f.read()\n",
        "\n",
        "\n",
        "  con = translator.translate(contents, dest='en')\n",
        "  contents = con.text\n",
        "\n",
        "\n",
        "  input_text =f'''Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "  ### Instruction:\n",
        "    {question} based on this text : {contents}.\n",
        "  ### Response:\n",
        "  '''\n",
        "\n",
        "  L = chat(input_text)\n",
        "  r = L.split(\"### Response:\")[-1][1:]\n",
        "\n",
        "  return r"
      ],
      "metadata": {
        "id": "9w4WSNZHuVV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(L.split(\"### Response:\")[-1][1:])"
      ],
      "metadata": {
        "id": "KhRbjgerUQ6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import random\n",
        "import time\n",
        "\n",
        "# Function to save the transcribed text into a text file\n",
        "def save_to_file(transcription, file_path):\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        file.write(transcription)\n",
        "\n",
        "# transcription code\n",
        "def get_text(url):\n",
        "    # speech-to-text transcription here\n",
        "    # ....\n",
        "    transcription = \"This is the speech-to-text transcription of the YouTube video from the URL: \" + url\n",
        "\n",
        "    # Save the transcription to a text file\n",
        "    file_path = \"transcription.txt\"\n",
        "    save_to_file(transcription, file_path)\n",
        "\n",
        "    return \"Transcription saved to file: \" + file_path\n",
        "\n",
        "# Function to save the transcribed text into a text file\n",
        "def save_to_file(transcription, file_path):\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        file.write(transcription)\n",
        "\n",
        "# Chatbot Interface\n",
        "def response(message, history):\n",
        "    L = chatBot(message)\n",
        "    L = translator.translate(L, dest='ar')\n",
        "    # For demonstration purposes, let's just return a random response from a list\n",
        "    return L.text\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "  with gr.Row():\n",
        "    with gr.Column(scale=1):\n",
        "          input=gr.Textbox(placeholder='Youtube video URL')\n",
        "          output=gr.Textbox(placeholder='Status', label='Status')\n",
        "          button=gr.Button(value=\"Submit\")\n",
        "          button.click(fn=get_text, inputs=input, outputs=output)\n",
        "\n",
        "\n",
        "    with gr.Column(scale=2):\n",
        "      chatbot = gr.Chatbot()\n",
        "      msg = gr.Textbox()\n",
        "      clear = gr.ClearButton([msg, chatbot])\n",
        "\n",
        "      def respond(message, chat_history):\n",
        "          bot_message = chatBot(message)\n",
        "          bot_message = translator.translate(bot_message, dest='ar')\n",
        "          bot_message = bot_message.text\n",
        "          #message#random.choice([\"How are you?\", \"I'm fine\", \"I'm very hungry\"])\n",
        "          chat_history.append((message, bot_message))\n",
        "          time.sleep(2)\n",
        "          return \"\", chat_history\n",
        "\n",
        "      def response(message, history):\n",
        "        L = chatBot(message)\n",
        "        L = translator.translate(L, dest='ar')\n",
        "        return L.text\n",
        "\n",
        "\n",
        "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
        "\n",
        "# Launch the grouped blocks\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "m8lTeOOYRaGP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}